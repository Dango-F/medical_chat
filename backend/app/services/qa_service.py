"""QA Service - Combines KG, Vector Search and LLM for Question Answering"""

import time
import uuid
import re
import asyncio
import json
from typing import List, Dict, Any, Optional, Tuple, AsyncGenerator
from loguru import logger

from app.core.config import settings
from app.models.query import (
    QueryRequest,
    QueryResponse,
    Evidence,
    KGPath,
    SourceType,
    AnswerSource
)
from app.services.kg_service import kg_service
from app.services.vector_service import vector_service
from app.services.memory_service import memory_service


class QAService:
    """Main service for medical question answering"""

    # å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°
    MAX_CONCURRENT_REQUESTS = 5
    # LLM è°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    LLM_TIMEOUT = 60

    # å¸¸è§åŒä¹‰è¯æˆ–å£è¯­åŒ–è¯æ˜ å°„åˆ°çŸ¥è¯†å›¾è°±ä¸­çš„è§„èŒƒåç§°
    SYNONYMS = {
        "å°å„¿éº»ç—¹ç—‡": "è„Šé«“ç°è´¨ç‚",
        "å°å„¿éº»ç—¹": "è„Šé«“ç°è´¨ç‚",
        "å„¿éº»ç—¹": "è„Šé«“ç°è´¨ç‚",
        "æ™®é€šæµæ„Ÿ": "æµæ„Ÿ",
        "æµæ„Ÿ": "æµè¡Œæ€§æ„Ÿå†’",
        "æ„Ÿå†’": "ä¸Šå‘¼å¸é“æ„ŸæŸ“",
    }

    def __init__(self):
        self.openai_client = None
        self.gemini_model = None
        self.siliconflow_client = None
        self._llm_provider = "mock"  # "openai", "gemini", "siliconflow", or "mock"
        self._semaphore: Optional[asyncio.Semaphore] = None

    async def initialize(self):
        """Initialize LLM client based on configuration"""
        # åˆå§‹åŒ–å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self._semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_REQUESTS)

        provider = settings.llm_provider.lower()

        # Try SiliconFlow (DeepSeek) first if configured
        if provider == "siliconflow" and settings.siliconflow_api_key:
            try:
                from openai import AsyncOpenAI
                self.siliconflow_client = AsyncOpenAI(
                    api_key=settings.siliconflow_api_key,
                    base_url=settings.siliconflow_base_url
                )
                self._llm_provider = "siliconflow"
                logger.info(
                    f"SiliconFlow client initialized (model: {settings.siliconflow_model})")
                return
            except Exception as e:
                logger.warning(f"Failed to initialize SiliconFlow: {e}")

        # Try Gemini if configured
        if provider == "gemini" and settings.gemini_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=settings.gemini_api_key)
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
                self._llm_provider = "gemini"
                logger.info("Gemini client initialized")
                return
            except Exception as e:
                logger.warning(f"Failed to initialize Gemini: {e}")

        # Try OpenAI if configured
        if provider == "openai" and settings.openai_api_key and settings.openai_api_key.startswith("sk-"):
            try:
                from openai import AsyncOpenAI
                self.openai_client = AsyncOpenAI(
                    api_key=settings.openai_api_key)
                self._llm_provider = "openai"
                logger.info("OpenAI client initialized")
                return
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAI: {e}")

        # Fallback to mock
        logger.info("No valid LLM API key provided. Using mock LLM responses.")
        self._llm_provider = "mock"

    async def _extract_entities_from_kg(self, query: str, history: list = None) -> List[str]:
        """ä»çŸ¥è¯†å›¾è°±ä¸­æå–å®ä½“ï¼ˆæ›´å‡†ç¡®ï¼‰ï¼Œå¹¶ç»“åˆä¼šè¯å†å²åšæ ¸å¿ƒæŒ‡ä»£æ¶ˆè§£ã€‚

        é€»è¾‘ï¼š
        1. å…ˆç”¨é™æ€è¯è¡¨æå–å½“å‰ query çš„å®ä½“
        2. å¦‚æœè¿æ¥ KGï¼Œåˆ™å¯¹å½“å‰ query ä¸­çš„å€™é€‰çŸ­è¯åš KG æ¨¡ç³Šæœç´¢
        3. å¦‚æœæœªèƒ½æå–åˆ°å®ä½“ï¼Œå°è¯•ä»å†å²æ¶ˆæ¯ä¸­å›æº¯æŸ¥æ‰¾å®ä½“ï¼ˆå¤„ç†ä»£è¯/çœç•¥ï¼‰
        4. æœ€åå†æŠŠå†å²ä¸ query åˆå¹¶å†æ¬¡åšä¸€æ¬¡æå–ï¼ˆä¿éšœè¦†ç›–ï¼‰
        """
        found_entities: List[str] = []

        # 1) å½“å‰ query çš„é™æ€è§„åˆ™æå–
        basic_entities = self._extract_entities(query)
        found_entities.extend(basic_entities)

        # 1b) åŒä¹‰è¯/å£è¯­åŒ–åŒ¹é…ï¼šä¼˜å…ˆæŠŠå¸¸è§å£è¯­æ˜ å°„åˆ°è§„èŒƒç–¾ç—…å
        for colloquial, canonical in self.SYNONYMS.items():
            if colloquial in query and canonical not in found_entities:
                found_entities.append(canonical)
                logger.debug(
                    f"Mapped colloquial '{colloquial}' to canonical '{canonical}' for query '{query}'")

        # 2) KG å¢å¼ºï¼šå½“å‰ query ä¸­å€™é€‰è¯çš„ KG æ¨¡ç³Š/å…¨æ–‡æ£€ç´¢
        if kg_service.is_connected:
            import re
            potential_terms = re.findall(r'[\u4e00-\u9fa5]{2,6}', query)

            for term in potential_terms:
                if term in found_entities:
                    continue
                diseases = await kg_service.search_disease(term, limit=1)
                if diseases:
                    found_entities.append(diseases[0])
                    continue
                symptoms = await kg_service.search_symptom(term, limit=1)
                if symptoms:
                    found_entities.append(symptoms[0])

        # 3) å¦‚æœå½“å‰ query æ²¡æœ‰æå–åˆ°å®ä½“ï¼Œå°è¯•ä»å†å²å›æº¯æœç´¢ï¼ˆè§£å†³ä»£è¯/çœç•¥é—®é¢˜ï¼‰
        if (not found_entities) and history:
            import re
            # ä¼˜å…ˆä»æœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯ä¸­å¯»æ‰¾å¯èƒ½çš„å®ä½“
            for msg in reversed(history[-6:]):
                # åªçœ‹ç”¨æˆ·å‘è¨€ä¼˜å…ˆ
                if msg.role != 'user':
                    continue
                terms = re.findall(r'[\u4e00-\u9fa5]{2,12}', msg.content)
                for term in terms:
                    if term in found_entities:
                        continue
                    diseases = await kg_service.search_disease(term, limit=1)
                    if diseases:
                        found_entities.append(diseases[0])
                        break
                    symptoms = await kg_service.search_symptom(term, limit=1)
                    if symptoms:
                        found_entities.append(symptoms[0])
                        break
                if found_entities:
                    break

        # 3b) å¦‚æœä»æœªæå–åˆ°å®ä½“ï¼Œå°è¯•æ›´æ¿€è¿›çš„ç›´æ¥ä»å½“å‰ query åšç–¾ç—…æ£€ç´¢ï¼ˆå¤„ç†åƒâ€œXX æ˜¯å•¥/æ˜¯ä»€ä¹ˆâ€çš„æƒ…å†µï¼‰
        if (not found_entities):
            import re
            # æ¸…ç†å¸¸è§é—®å¥åç¼€
            cleaned = re.sub(
                r'(æ˜¯ä»€ä¹ˆ|æ˜¯å•¥|å•¥æ˜¯|æ˜¯å•¥æ„æ€|æ˜¯ä»€ä¹ˆæ„æ€|æ˜¯ä»€ä¹ˆç—…|æ€ä¹ˆå›äº‹|æœ‰å“ªäº›ç—‡çŠ¶|ç—‡çŠ¶|æ€ä¹ˆåŠ)$', '', query.strip())
            cleaned = cleaned.strip()

            # å…ˆå°è¯•ç”¨æ•´ä¸ªæ¸…ç†åçš„ query å»åšç–¾ç—…æœç´¢
            if cleaned:
                diseases = await kg_service.search_disease(cleaned, limit=3)
                if diseases:
                    found_entities.extend(diseases)

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œåšä¸€ä¸ªç®€æ˜“çš„ n-gram æ‰«æï¼ˆé•¿åˆ°çŸ­ï¼‰ï¼Œå°½é‡æ‰¾åˆ°æ›´å…·ä½“çš„ç–¾ç—…å
            if not found_entities:
                text = re.findall(r'[\u4e00-\u9fa5]+', query)
                text = ''.join(text)  # åªä¿ç•™ä¸­æ–‡å­—ç¬¦
                for length in range(6, 1, -1):
                    for i in range(0, max(0, len(text) - length + 1)):
                        sub = text[i:i+length]
                        if sub in found_entities:
                            continue
                        diseases = await kg_service.search_disease(sub, limit=1)
                        if diseases:
                            found_entities.append(diseases[0])
                            break
                    if found_entities:
                        break

            # è®°å½•å‘ç°ä»¥ä¾¿è°ƒè¯•
            if found_entities:
                logger.debug(
                    f"Fallback disease match from query '{query}': {found_entities}")

        # 4) æœ€åä¸€æ­¥ï¼šæŠŠå†å²æ–‡æœ¬ä¸å½“å‰ query åˆå¹¶å†åšä¸€æ¬¡é™æ€ + KG æ£€ç´¢ï¼ˆè¡¥å¿æ€§ï¼‰
        if history and kg_service.is_connected:
            combined_text = ' '.join(
                [m.content for m in history[-6:]]) + ' ' + query
            more_entities = self._extract_entities(combined_text)
            import re
            potential_terms = re.findall(
                r'[\u4e00-\u9fa5]{2,6}', combined_text)

            for term in potential_terms:
                if term in found_entities:
                    continue
                diseases = await kg_service.search_disease(term, limit=1)
                if diseases:
                    found_entities.append(diseases[0])
                    continue
                symptoms = await kg_service.search_symptom(term, limit=1)
                if symptoms:
                    found_entities.append(symptoms[0])

            for ent in more_entities:
                if ent not in found_entities:
                    found_entities.append(ent)

        # preserve order, remove duplicates
        return list(dict.fromkeys(found_entities))

    def _extract_entities(self, query: str) -> List[str]:
        """Extract medical entities from query using simple pattern matching"""
        # Common medical terms to look for
        medical_terms = [
            "å¤´ç—›", "åå¤´ç—›", "ç´§å¼ æ€§å¤´ç—›", "å‘çƒ­", "å‘çƒ§", "æ„Ÿå†’", "æµæ„Ÿ",
            "å’³å—½", "æ¶å¿ƒ", "å‘•å", "è…¹ç—›", "è…¹æ³»", "ä¾¿ç§˜", "èƒ¸ç—›", "å¿ƒæ‚¸",
            "é«˜è¡€å‹", "ç³–å°¿ç—…", "å“®å–˜", "è¿‡æ•", "çš®ç–¹", "å¤±çœ ", "ç„¦è™‘", "æŠ‘éƒ",
            "å¸ƒæ´›èŠ¬", "å¯¹ä¹™é…°æ°¨åŸºé…š", "é˜¿å¸åŒ¹æ—", "æŠ—ç”Ÿç´ ", "ç»´ç”Ÿç´ ",
            "è„‘è†œç‚", "è„‘å’ä¸­", "ä¸­é£", "ç™«ç—«", "å¸•é‡‘æ£®",
            "ç•å…‰", "é¢ˆéƒ¨åƒµç¡¬", "æ„è¯†", "è§†åŠ›", "ä¹åŠ›", "ç–²åŠ³",
            "è‚ºç‚", "æ”¯æ°”ç®¡ç‚", "èƒƒç‚", "è‚ ç‚", "è‚ç‚", "è‚¾ç‚",
            "å† å¿ƒç—…", "å¿ƒè‚Œæ¢—æ­»", "å¿ƒç»ç—›", "å¿ƒå¾‹å¤±å¸¸",
            "éª¨æŠ˜", "å…³èŠ‚ç‚", "è…°ç—›", "é¢ˆæ¤ç—…", "è‚©å‘¨ç‚",
            "æ¹¿ç–¹", "è¨éº»ç–¹", "ç—¤ç–®", "é“¶å±‘ç—…",
            "è´«è¡€", "ç™½è¡€ç—…", "æ·‹å·´ç˜¤"
        ]

        found_entities = []

        for term in medical_terms:
            if term in query:
                found_entities.append(term)

        return found_entities

    async def _extract_entities_from_query_only(self, query: str) -> List[str]:
        """Extract entities using only the current query (no history) with KG enhancement.

        This ensures evidence retrieval is scoped to the current user question, avoiding
        pulling in documents that were relevant only to earlier turns in the conversation.
        """
        found_entities: List[str] = []

        # Basic extraction from current query
        basic_entities = self._extract_entities(query)
        found_entities.extend(basic_entities)

        # KG-enhanced search using current query terms (no history)
        if kg_service.is_connected:
            import re
            potential_terms = re.findall(r'[\u4e00-\u9fa5]{2,6}', query)

            for term in potential_terms:
                if term in found_entities:
                    continue
                diseases = await kg_service.search_disease(term, limit=1)
                if diseases:
                    found_entities.append(diseases[0])
                    continue
                symptoms = await kg_service.search_symptom(term, limit=1)
                if symptoms:
                    found_entities.append(symptoms[0])

        # preserve order, remove duplicates
        return list(dict.fromkeys(found_entities))

    def _build_history_context(self, history: list = None) -> str:
        """æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡"""
        if not history or len(history) == 0:
            return ""

        history_context = "\n**å¯¹è¯å†å²**ï¼š\n"
        for msg in history[-6:]:  # åªä¿ç•™æœ€è¿‘6è½®å¯¹è¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            role_name = "ç”¨æˆ·" if msg.role == "user" else "åŠ©æ‰‹"
            history_context += f"{role_name}ï¼š{msg.content}\n"
        history_context += "\n"
        return history_context

    def _build_llm_prompt(
        self,
        query: str,
        kg_context: str,
        evidence_context: str,
        history: list = None
    ) -> str:
        """Build the prompt for LLM with grounding instructions (æœ‰çŸ¥è¯†å›¾è°±æ•°æ®æ—¶ä½¿ç”¨)"""
        history_context = self._build_history_context(history)

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**é‡è¦è§„åˆ™**ï¼š
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†å›¾è°±ä¸­æä¾›çš„åŒ»å­¦ä¿¡æ¯æ¥å›ç­”é—®é¢˜
2. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šï¼Œä½†è¡¨è¾¾è¦é€šä¿—æ˜“æ‡‚
3. å¦‚æœçŸ¥è¯†å›¾è°±ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¸€å®šæ®æ­¤å›ç­”ï¼›å¦‚æœæ²¡æœ‰ï¼Œè¯·è¯´æ˜"æš‚æ— ç›¸å…³ä¿¡æ¯"ï¼Œå¹¶ç»™å‡ºåˆç†çš„å»ºè®®ã€‚
4. å§‹ç»ˆæé†’ç”¨æˆ·æœ¬ç³»ç»Ÿä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£åŒ»ç”Ÿè¯Šæ–­
5. å¯¹äºå±é™©ä¿¡å·ï¼ˆå¦‚å‰§çƒˆå¤´ç—›ã€é«˜çƒ­ã€æ„è¯†æ”¹å˜ã€èƒ¸ç—›ï¼‰ï¼Œè¦å¼ºè°ƒç«‹å³å°±åŒ»
6. å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼ˆå¦‚ä»£è¯æŒ‡ä»£ã€çœç•¥çš„ä¸»è¯­ç­‰ï¼‰
7. ä¸€äº›åŸºæœ¬ä¿¡æ¯ä½ æ˜¯å¯ä»¥å›å¤çš„ï¼Œæ¯”å¦‚æ—¥æœŸç­‰ã€‚

**åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯**ï¼š
{kg_context}
{history_context}
**å½“å‰ç”¨æˆ·é—®é¢˜**ï¼š
{query}

å¦‚æœç”¨æˆ·æé—®çš„æ˜¯åŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œè¯·æä¾›ç»“æ„åŒ–çš„å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
1. ç®€è¦å›ç­”ï¼ˆæ¦‚æ‹¬ä¸»è¦ä¿¡æ¯ï¼‰
2. è¯¦ç»†è¯´æ˜ï¼ˆåˆ†ç‚¹åˆ—å‡ºç—‡çŠ¶/æ²»ç–—/é¢„é˜²ç­‰ç›¸å…³ä¿¡æ¯ï¼‰
3. å°±åŒ»å»ºè®®ï¼ˆä½•æ—¶éœ€è¦å°±åŒ»ï¼Œçœ‹ä»€ä¹ˆç§‘å®¤ï¼‰
4. æ³¨æ„äº‹é¡¹ï¼ˆé¥®é£Ÿã€ç”¨è¯ç­‰ï¼‰
å¦åˆ™ä¸ç”¨æä¾›ç»“æ„åŒ–å›ç­”ï¼Œç®€è¦å›ç­”å³å¯ã€‚

å›ç­”ï¼š"""

    def _build_llm_prompt_without_kg(
        self,
        query: str,
        history: list = None
    ) -> str:
        """å½“çŸ¥è¯†å›¾è°±æ— æ•°æ®æ—¶ï¼Œæ„å»ºçº¯ LLM çš„ prompt"""
        history_context = self._build_history_context(history)

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚

**é‡è¦è¯´æ˜**ï¼š
å½“å‰åŒ»ç–—çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ï¼Œè¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†æä¾›å‚è€ƒå»ºè®®ã€‚

**å›ç­”è¦æ±‚**ï¼š
1. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šï¼Œä½†è¡¨è¾¾è¦é€šä¿—æ˜“æ‡‚
2. å§‹ç»ˆå¼ºè°ƒæœ¬å›ç­”ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—
3. å¯¹äºå±é™©ä¿¡å·ï¼ˆå¦‚å‰§çƒˆå¤´ç—›ã€é«˜çƒ­ä¸é€€ã€æ„è¯†æ”¹å˜ã€èƒ¸ç—›ã€å‘¼å¸å›°éš¾ç­‰ï¼‰ï¼Œè¦å¼ºè°ƒç«‹å³å°±åŒ»
4. ä¸è¦åœ¨å›ç­”ä¸­æåŠ"çŸ¥è¯†å›¾è°±"ï¼Œç›´æ¥ç»™å‡ºä¸“ä¸šå»ºè®®å³å¯
5. å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çš„é—®é¢˜
{history_context}
**ç”¨æˆ·é—®é¢˜**ï¼š
{query}

å¦‚æœç”¨æˆ·æé—®çš„æ˜¯åŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œè¯·æä¾›ç»“æ„åŒ–çš„å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
1. ç®€è¦å›ç­”ï¼ˆæ¦‚æ‹¬ä¸»è¦ä¿¡æ¯ï¼‰
2. è¯¦ç»†è¯´æ˜ï¼ˆåˆ†ç‚¹åˆ—å‡ºç—‡çŠ¶/æ²»ç–—/é¢„é˜²ç­‰ç›¸å…³ä¿¡æ¯ï¼‰
3. å°±åŒ»å»ºè®®ï¼ˆä½•æ—¶éœ€è¦å°±åŒ»ï¼Œçœ‹ä»€ä¹ˆç§‘å®¤ï¼‰
4. æ³¨æ„äº‹é¡¹ï¼ˆé¥®é£Ÿã€ç”¨è¯ç­‰ï¼‰
å¦åˆ™ä¸ç”¨æä¾›ç»“æ„åŒ–å›ç­”ï¼Œç®€è¦å›ç­”å³å¯ã€‚

å›ç­”ï¼š"""

    async def _generate_kg_based_response(
        self,
        query: str,
        entities: List[str],
        kg_context: str
    ) -> str:
        """åŸºäºçŸ¥è¯†å›¾è°±ç”Ÿæˆå›ç­”"""
        if not kg_context:
            return await self._generate_fallback_response(query, entities)

        # æ„å»ºåŸºäºçŸ¥è¯†å›¾è°±çš„å›ç­”
        response = f"## å…³äºæ‚¨çš„é—®é¢˜\n\næ ¹æ®åŒ»ç–—çŸ¥è¯†åº“çš„ä¿¡æ¯ï¼Œä¸ºæ‚¨æä¾›ä»¥ä¸‹å‚è€ƒï¼š\n\n{kg_context}\n"
        response += "\n---\nğŸ“š **æç¤º**ï¼šæœ¬å›ç­”åŸºäºåŒ»ç–—çŸ¥è¯†å›¾è°±ç”Ÿæˆï¼Œæœªä½¿ç”¨AIå¤§æ¨¡å‹ã€‚\n"
        response += "âš ï¸ **é‡è¦æç¤º**ï¼šä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"

        return response

    async def _generate_fallback_response(self, query: str, entities: List[str]) -> str:
        """å½“æ²¡æœ‰çŸ¥è¯†å›¾è°±æ•°æ®æ—¶çš„å¤‡ç”¨å›ç­”"""
        return f"""## å…³äºæ‚¨çš„é—®é¢˜

æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ã€‚

ç›®å‰çŸ¥è¯†åº“ä¸­æš‚æ— å…³äº"{', '.join(entities) if entities else 'æ‚¨æ‰€è¯¢é—®å†…å®¹'}"çš„è¯¦ç»†ä¿¡æ¯ã€‚

**å»ºè®®**ï¼š
1. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„åŒ»å­¦æœ¯è¯­è¿›è¡ŒæŸ¥è¯¢
2. å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å‰å¾€åŒ»é™¢å°±è¯Š
3. å¯ä»¥å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®çš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®

---
ğŸ“š **æç¤º**ï¼šæœ¬å›ç­”åŸºäºåŒ»ç–—çŸ¥è¯†å›¾è°±ç”Ÿæˆï¼Œæœªä½¿ç”¨AIå¤§æ¨¡å‹ã€‚
âš ï¸ **é‡è¦æç¤º**ï¼šæœ¬ç³»ç»Ÿä»…ä¾›åŒ»ç–—ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚"""

    def _generate_mock_response(
        self,
        query: str,
        entities: List[str],
        evidence: List[Evidence],
        kg_paths: List[KGPath],
        kg_context: str = ""
    ) -> str:
        """Generate a mock response based on retrieved evidence"""
        # å¤‡ç”¨æ–¹æ¡ˆæç¤ºä¿¡æ¯
        fallback_notice = "\n\n---\nğŸ“š **æç¤º**ï¼šæœ¬å›ç­”åŸºäºåŒ»ç–—çŸ¥è¯†å›¾è°±ç”Ÿæˆï¼Œæœªä½¿ç”¨AIå¤§æ¨¡å‹ã€‚\n"

        # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆä½¿ç”¨
        if kg_context:
            response = f"## å…³äºæ‚¨çš„é—®é¢˜\n\næ ¹æ®åŒ»ç–—çŸ¥è¯†åº“çš„ä¿¡æ¯ï¼Œä¸ºæ‚¨æä¾›ä»¥ä¸‹å‚è€ƒï¼š\n\n{kg_context}\n"
            response += fallback_notice
            response += "âš ï¸ **é‡è¦æç¤º**ï¼šä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"
            return response

        # Check for headache-related queries
        if any(term in query for term in ["å¤´ç—›", "å¤´ç–¼", "åå¤´ç—›"]):
            return self._generate_headache_response(query, evidence, kg_paths) + fallback_notice

        # Check for fever-related queries
        if any(term in query for term in ["å‘çƒ­", "å‘çƒ§", "ä½“æ¸©"]):
            return self._generate_fever_response(query, evidence) + fallback_notice

        # Check for drug-related queries
        if any(term in query for term in ["è¯", "ç”¨è¯", "åƒä»€ä¹ˆè¯", "å¸ƒæ´›èŠ¬", "å¯¹ä¹™é…°æ°¨åŸºé…š"]):
            return self._generate_drug_response(query, evidence) + fallback_notice

        # Check for diabetes
        if any(term in query for term in ["ç³–å°¿ç—…", "è¡€ç³–"]):
            return self._generate_diabetes_response(query, evidence) + fallback_notice

        # Check for hypertension
        if any(term in query for term in ["é«˜è¡€å‹", "è¡€å‹"]):
            return self._generate_hypertension_response(query, evidence) + fallback_notice

        # Default response
        return self._generate_default_response(query, evidence) + fallback_notice

    def _generate_headache_response(
        self,
        query: str,
        evidence: List[Evidence],
        kg_paths: List[KGPath]
    ) -> str:
        """Generate response for headache-related queries"""
        response = """## å¤´ç—›çš„å¯èƒ½åŸå› åˆ†æ

æ ¹æ®æ‚¨æè¿°çš„ç—‡çŠ¶ï¼Œå¤´ç—›å¯èƒ½ç”±ä»¥ä¸‹å‡ ç§å¸¸è§åŸå› å¼•èµ·ï¼š

### å¸¸è§åŸå› 

1. **åå¤´ç—›** [æ¥æº: ä¸­åç¥ç»ç§‘æ‚å¿—, PMID:34567890]
   - è¡¨ç°ä¸ºåå¤å‘ä½œçš„ä¸­é‡åº¦æåŠ¨æ€§å¤´ç—›
   - å¸¸ä¼´æœ‰æ¶å¿ƒã€å‘•åã€ç•å…‰å’Œç•å£°
   - å‘ä½œé€šå¸¸æŒç»­4-72å°æ—¶

2. **ç´§å¼ æ€§å¤´ç—›** [æ¥æº: Headache, PMID:34567891]
   - æœ€å¸¸è§çš„å¤´ç—›ç±»å‹ï¼Œç»ˆç”Ÿæ‚£ç—…ç‡è¾¾78%
   - è¡¨ç°ä¸ºåŒä¾§å‹è¿«æ€§æˆ–ç´§ç®æ ·å¤´ç—›
   - ç¨‹åº¦è½»è‡³ä¸­åº¦ï¼Œä¸å› æ—¥å¸¸æ´»åŠ¨åŠ é‡

3. **ä¸Šå‘¼å¸é“æ„ŸæŸ“ï¼ˆæ„Ÿå†’/æµæ„Ÿï¼‰**
   - å°¤å…¶åœ¨ä¼´æœ‰å‘çƒ­ã€å’³å—½ã€æµæ¶•æ—¶éœ€è€ƒè™‘
   - å¤´ç—›é€šå¸¸ä¸ºå…¨å¤´éƒ¨é’ç—›

### âš ï¸ éœ€è¦ç«‹å³å°±åŒ»çš„å±é™©ä¿¡å· [æ¥æº: NICEä¸´åºŠæŒ‡å—]

å¦‚å‡ºç°ä»¥ä¸‹æƒ…å†µï¼Œè¯·**ç«‹å³**å‰å¾€åŒ»é™¢å°±è¯Šï¼š
- **é›·å‡»æ ·å¤´ç—›**ï¼šæ•°ç§’å†…è¾¾åˆ°é«˜å³°çš„å‰§çƒˆå¤´ç—›
- **ä¼´å‘çƒ­å’Œé¢ˆéƒ¨åƒµç¡¬**ï¼šå¯èƒ½æç¤ºè„‘è†œç‚
- **æ„è¯†æ”¹å˜æˆ–ç¥ç»åŠŸèƒ½ç¼ºæŸ**
- **å¤´ç—›è¿›è¡Œæ€§åŠ é‡**
- **50å²ä»¥åæ–°å‘å¤´ç—›**
- **ä¼´è§†åŠ›æ”¹å˜æˆ–çœ¼ç—›**

### å»ºè®®

1. ä¿æŒå……è¶³ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦åŠ³ç´¯
2. å¯è€ƒè™‘å¯¹ç—‡æœç”¨å¯¹ä¹™é…°æ°¨åŸºé…šï¼ˆ1000mgï¼‰æˆ–å¸ƒæ´›èŠ¬ï¼ˆ400-600mgï¼‰ç¼“è§£ç—‡çŠ¶
3. å¦‚å¤´ç—›æŒç»­è¶…è¿‡3å¤©æˆ–é¢‘ç¹å‘ä½œï¼Œå»ºè®®å°±åŒ»è¿›ä¸€æ­¥è¯„ä¼°
4. è®°å½•å¤´ç—›æ—¥è®°ï¼ˆå‘ä½œæ—¶é—´ã€æŒç»­æ—¶é—´ã€è¯±å› ã€ä¼´éšç—‡çŠ¶ï¼‰æœ‰åŠ©äºè¯Šæ–­"""

        return response

    def _generate_fever_response(self, query: str, evidence: List[Evidence]) -> str:
        """Generate response for fever-related queries"""
        return """## å‘çƒ­çš„è¯„ä¼°ä¸å»ºè®®

### å‘çƒ­å®šä¹‰
å‘çƒ­å®šä¹‰ä¸ºæ ¸å¿ƒä½“æ¸©â‰¥38Â°Cï¼ˆè…‹æ¸©â‰¥37.3Â°Cå¯è§†ä¸ºä½çƒ­ï¼‰ã€‚[æ¥æº: ä¸­åå†…ç§‘æ‚å¿—, PMID:34567893]

### å¸¸è§åŸå› 
1. **æ„ŸæŸ“æ€§ç–¾ç—…**ï¼ˆæœ€å¸¸è§ï¼‰
   - ä¸Šå‘¼å¸é“æ„ŸæŸ“ã€æµæ„Ÿ
   - è‚ºç‚ã€æ³Œå°¿é“æ„ŸæŸ“ç­‰

2. **éæ„ŸæŸ“æ€§åŸå› **
   - è‡ªèº«å…ç–«ç–¾ç—…
   - è¯ç‰©çƒ­

### âš ï¸ éœ€è¦å°±åŒ»çš„æƒ…å†µ
- ä½“æ¸©â‰¥39Â°CæŒç»­24å°æ—¶ä»¥ä¸Š
- ä¼´æœ‰å‰§çƒˆå¤´ç—›å’Œé¢ˆéƒ¨åƒµç¡¬ï¼ˆè­¦æƒ•è„‘è†œç‚ï¼‰
- ä¼´æœ‰æ„è¯†æ”¹å˜
- ä¼´æœ‰çš®ç–¹
- å„¿ç«¥ã€è€å¹´äººæˆ–å…ç–«åŠ›ä½ä¸‹è€…

### å¯¹ç—‡å¤„ç†å»ºè®®
1. å¤šé¥®æ°´ï¼Œä¿æŒä¼‘æ¯
2. å¯æœç”¨å¯¹ä¹™é…°æ°¨åŸºé…šæˆ–å¸ƒæ´›èŠ¬é€€çƒ­
3. ç‰©ç†é™æ¸©ï¼ˆæ¸©æ°´æ“¦æµ´ï¼‰
4. å¦‚æŒç»­ä¸é€€æˆ–ä¼´æœ‰å…¶ä»–ç—‡çŠ¶ï¼Œè¯·åŠæ—¶å°±åŒ»"""

    def _generate_drug_response(self, query: str, evidence: List[Evidence]) -> str:
        """Generate response for drug-related queries"""
        return """## è¯ç‰©ä¿¡æ¯

### å¸¸ç”¨æ­¢ç—›é€€çƒ­è¯ç‰© [æ¥æº: DrugBank, Cochrane]

1. **å¯¹ä¹™é…°æ°¨åŸºé…šï¼ˆæ‰‘çƒ­æ¯ç—›ï¼‰**
   - ç”¨æ³•ï¼šæˆäººæ¯æ¬¡500-1000mgï¼Œæ¯4-6å°æ—¶ä¸€æ¬¡
   - æ¯æ—¥æœ€å¤§å‰‚é‡ï¼š4000mg
   - é€‚ç”¨äºè½»è‡³ä¸­åº¦ç–¼ç—›å’Œé€€çƒ­
   - æ³¨æ„ï¼šé¿å…è¿‡é‡ï¼Œæœ‰è‚æŸå®³é£é™©

2. **å¸ƒæ´›èŠ¬** [æ¥æº: DrugBank DB01050]
   - ç”¨æ³•ï¼šæˆäººæ¯æ¬¡200-400mgï¼Œæ¯4-6å°æ—¶ä¸€æ¬¡
   - æ¯æ—¥æœ€å¤§å‰‚é‡ï¼š1200mgï¼ˆéå¤„æ–¹ï¼‰
   - åŒæ—¶å…·æœ‰æ­¢ç—›ã€é€€çƒ­ã€æŠ—ç‚ä½œç”¨
   - ç¦å¿Œï¼šæ´»åŠ¨æ€§æ¶ˆåŒ–é“æºƒç–¡ã€ä¸¥é‡å¿ƒè¡°

### âš ï¸ ç”¨è¯æ³¨æ„äº‹é¡¹
- æ¯æœˆä½¿ç”¨æ­¢ç—›è¯ä¸å®œè¶…è¿‡10å¤©ï¼Œä»¥é˜²è¯ç‰©è¿‡åº¦ä½¿ç”¨æ€§å¤´ç—›
- æœ‰èƒƒç—…å²è€…æ…ç”¨NSAIDsç±»è¯ç‰©
- è‚è‚¾åŠŸèƒ½ä¸å…¨è€…è¯·éµåŒ»å˜±è°ƒæ•´å‰‚é‡
- å¦‚éœ€é•¿æœŸç”¨è¯ï¼Œè¯·å’¨è¯¢åŒ»ç”Ÿ"""

    def _generate_diabetes_response(self, query: str, evidence: List[Evidence]) -> str:
        """Generate response for diabetes-related queries"""
        return """## ç³–å°¿ç—…ç›¸å…³ä¿¡æ¯ [æ¥æº: ä¸­åç³–å°¿ç—…æ‚å¿— 2024å¹´æŒ‡å—]

### 2å‹ç³–å°¿ç—…ç®¡ç†è¦ç‚¹

**æ§åˆ¶ç›®æ ‡**ï¼š
- HbA1c < 7%ï¼ˆå¯æ ¹æ®ä¸ªä½“æƒ…å†µè°ƒæ•´ï¼‰
- ç©ºè…¹è¡€ç³–ï¼š4.4-7.0 mmol/L
- é¤å2å°æ—¶è¡€ç³–ï¼š< 10.0 mmol/L

**ä¸€çº¿ç”¨è¯**ï¼š
- äºŒç”²åŒèƒæ˜¯2å‹ç³–å°¿ç—…é¦–é€‰è¯ç‰©
- æ— ç¦å¿Œç—‡æ‚£è€…åº”ä»è¯Šæ–­æ—¶å¼€å§‹ä½¿ç”¨

**ç”Ÿæ´»æ–¹å¼å¹²é¢„**ï¼š
1. é¥®é£Ÿæ§åˆ¶ï¼šæ§åˆ¶æ€»çƒ­é‡ï¼Œå‡è¡¡è¥å…»
2. è§„å¾‹è¿åŠ¨ï¼šæ¯å‘¨è‡³å°‘150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦è¿åŠ¨
3. æˆ’çƒŸé™é…’
4. æ§åˆ¶ä½“é‡

**å®šæœŸç›‘æµ‹**ï¼š
- è¡€ç³–ç›‘æµ‹
- æ¯3-6ä¸ªæœˆæ£€æµ‹HbA1c
- æ¯å¹´çœ¼åº•æ£€æŸ¥
- æ¯å¹´è‚¾åŠŸèƒ½æ£€æŸ¥
- å®šæœŸè¶³éƒ¨æ£€æŸ¥

âš ï¸ ç³–å°¿ç—…ç®¡ç†éœ€è¦ä¸ªä½“åŒ–æ–¹æ¡ˆï¼Œè¯·éµåŒ»å˜±æ²»ç–—ã€‚"""

    def _generate_hypertension_response(self, query: str, evidence: List[Evidence]) -> str:
        """Generate response for hypertension-related queries"""
        return """## é«˜è¡€å‹ç›¸å…³ä¿¡æ¯ [æ¥æº: ä¸­å›½é«˜è¡€å‹é˜²æ²»æŒ‡å— 2023]

### è¯Šæ–­æ ‡å‡†
éåŒæ—¥3æ¬¡è¡€å‹æµ‹é‡â‰¥140/90mmHgå³å¯è¯Šæ–­é«˜è¡€å‹ã€‚

### æ²»ç–—ç›®æ ‡
- ä¸€èˆ¬æ‚£è€…ï¼š< 140/90 mmHg
- é«˜å±æ‚£è€…ï¼š< 130/80 mmHg

### ä¸€çº¿é™å‹è¯ç‰©
1. ACEI/ARBï¼ˆæ™®åˆ©ç±»/æ²™å¦ç±»ï¼‰
2. CCBï¼ˆåœ°å¹³ç±»ï¼‰
3. åˆ©å°¿å‰‚
4. Î²å—ä½“é˜»æ»å‰‚

### ç”Ÿæ´»æ–¹å¼æ”¹å˜
1. **é™ç›**ï¼šæ¯æ—¥æ‘„ç›<6g
2. **å‡é‡**ï¼šBMIæ§åˆ¶åœ¨24ä»¥ä¸‹
3. **æˆ’çƒŸ**ï¼šå®Œå…¨æˆ’çƒŸ
4. **é™é…’**ï¼šç”·æ€§<25g/å¤©ï¼Œå¥³æ€§<15g/å¤©
5. **è¿åŠ¨**ï¼šæ¯å‘¨5-7å¤©ï¼Œæ¯æ¬¡30åˆ†é’Ÿæœ‰æ°§è¿åŠ¨

### âš ï¸ æ³¨æ„äº‹é¡¹
- é«˜è¡€å‹éœ€è¦é•¿æœŸç®¡ç†ï¼Œä¸å¯è‡ªè¡Œåœè¯
- è¡€å‹æ³¢åŠ¨å¤§æˆ–æ§åˆ¶ä¸ä½³è¯·åŠæ—¶å°±åŒ»
- å®šæœŸç›‘æµ‹è¡€å‹å¹¶è®°å½•"""

    def _generate_default_response(self, query: str, evidence: List[Evidence]) -> str:
        """Generate a default response when no specific pattern matches"""
        evidence_summary = ""
        if evidence:
            evidence_summary = "\n\n### ç›¸å…³å‚è€ƒèµ„æ–™\n"
            for i, ev in enumerate(evidence[:3], 1):
                evidence_summary += f"{i}. {ev.section or 'åŒ»å­¦æ–‡çŒ®'} [æ¥æº: {ev.source}]\n"

        return f"""## å…³äºæ‚¨çš„é—®é¢˜

æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ã€‚æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæˆ‘æ£€ç´¢äº†ç›¸å…³çš„åŒ»å­¦èµ„æ–™ã€‚

ç”±äºé—®é¢˜çš„å…·ä½“æ€§ï¼Œå»ºè®®æ‚¨ï¼š

1. **è¯¦ç»†æè¿°ç—‡çŠ¶**ï¼šåŒ…æ‹¬æŒç»­æ—¶é—´ã€ä¸¥é‡ç¨‹åº¦ã€ä¼´éšç—‡çŠ¶ç­‰
2. **å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ**ï¼šè·å–é’ˆå¯¹æ€§çš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®
3. **ä¸è¦è‡ªè¡Œç”¨è¯**ï¼šç‰¹åˆ«æ˜¯å¤„æ–¹è¯ç‰©

{evidence_summary}

âš ï¸ æœ¬ç³»ç»Ÿä»…ä¾›ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"""

    async def process_query(self, request: QueryRequest) -> QueryResponse:
        """Process a medical question and return structured answer with evidence"""
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼Œé˜²æ­¢åŒæ—¶å¤„ç†å¤ªå¤šè¯·æ±‚
        if self._semaphore is None:
            self._semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_REQUESTS)

        async with self._semaphore:
            return await self._process_query_internal(request)

    async def _process_query_internal(self, request: QueryRequest) -> QueryResponse:
        """Internal method to process a query with concurrency control"""
        start_time = time.time()
        query_id = f"q_{uuid.uuid4().hex[:12]}"

        logger.info(f"Processing query [{query_id}]: {request.query[:50]}...")

        # Step 1: Extract entities from query (ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼º)ï¼Œå¹¶ç»“åˆå†å²åšæŒ‡ä»£æ¶ˆè§£
        entities = await self._extract_entities_from_kg(request.query, request.history)
        logger.debug(f"Extracted entities: {entities}")

        # Step 2: Retrieve from Knowledge Graph
        kg_paths = []
        if request.include_kg_paths and entities:
            kg_paths = await kg_service.find_paths_for_query(entities)
            logger.debug(f"Found {len(kg_paths)} KG paths")

        # Step 3: Retrieve from Vector Store (documents/literature)
        evidence = []
        if request.include_evidence:
            # Use entities extracted from the current query only to avoid pulling evidence from prior turns
            current_entities = await self._extract_entities_from_query_only(request.query)
            evidence = await vector_service.search_documents(
                request.query,
                keywords=current_entities,
                limit=request.max_answers + 2
            )
            logger.debug(
                f"Extracted current-only entities for evidence: {current_entities}")
            logger.debug(f"Found {len(evidence)} evidence documents")

        # Step 4: Search user memory (semantic-like) and Build context for LLM (ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼º)
        kg_context = ""

        # é¦–å…ˆæ£€ç´¢ç”¨æˆ·è®°å¿†ï¼ˆå¦‚æœæœ‰ï¼‰ï¼ŒæŠŠé«˜ç›¸å…³è®°å¿†ä½œä¸ºè¡¥å……ä¸Šä¸‹æ–‡
        memory_results = []
        try:
            if request.user_id:
                memory_results = await memory_service.search_memory(request.query, user_id=request.user_id, top_k=5)
                if memory_results:
                    mem_text = "ç”¨æˆ·å†å²è®°å¿†ï¼š\n"
                    for m in memory_results:
                        mem_text += f"- ({round(m.get('score',0),2)}) {m.get('content')}\n"
                    kg_context += mem_text + "\n"
        except Exception as e:
            logger.debug(f"Memory search failed: {e}")

        if entities and kg_service.is_connected:
            # ä»çŸ¥è¯†å›¾è°±è·å–è¯¦ç»†ä¸Šä¸‹æ–‡
            kg_context += await kg_service.get_kg_context_for_query(entities)

        if not kg_context and kg_paths:
            # å¤‡ç”¨ï¼šä»è·¯å¾„æ„å»ºä¸Šä¸‹æ–‡
            kg_context = kg_context or "ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š\n"
            for path in kg_paths[:3]:
                for node in path.nodes:
                    kg_context += f"- {node.type}: {node.label}"
                    if node.properties.get("description"):
                        kg_context += f" - {node.properties['description']}"
                    kg_context += "\n"

        evidence_context = ""
        if evidence:
            evidence_context = "åŒ»å­¦æ–‡çŒ®è¯æ®ï¼š\n"
            for ev in evidence[:5]:
                evidence_context += f"- [{ev.source}] {ev.snippet}\n"

        # å°†è®°å¿†æ£€ç´¢ç»“æœä¹ŸåŠ å…¥è¯æ®ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if memory_results:
            evidence_context += "\næ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†ï¼š\n"
            for m in memory_results:
                evidence_context += f"- {m.get('content')}\n"

        # Step 5: Generate answer
        # åˆ¤æ–­çŸ¥è¯†å›¾è°±æ˜¯å¦æœ‰æ•°æ®
        kg_available = bool(kg_context or kg_paths)

        # æ„å»ºæ¶ˆæ¯å†å²åˆ—è¡¨ï¼ˆç”¨äºæ”¯æŒä¸Šä¸‹æ–‡çš„ LLM è°ƒç”¨ï¼‰
        history_messages = []
        if request.history:
            for msg in request.history[-6:]:  # é™åˆ¶å†å²é•¿åº¦
                history_messages.append(
                    {"role": msg.role, "content": msg.content})

        # è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆç”¨äºæ ‡æ³¨ï¼‰
        model_name_map = {
            "mock": "æ¨¡æ¿å›å¤",
            "gemini": "Gemini",
            "openai": "GPT-4",
            "siliconflow": settings.siliconflow_model
        }
        current_model_name = model_name_map.get(self._llm_provider, "AI")

        # æ— çŸ¥è¯†å›¾è°±æ•°æ®æ—¶çš„æ¥æºæ ‡æ³¨
        no_kg_notice = f"""

---
ğŸ¤– **æ¥æºè¯´æ˜**ï¼šçŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæœ¬å›ç­”ç”± AI å¤§æ¨¡å‹ï¼ˆ{current_model_name}ï¼‰åŸºäºé€šç”¨åŒ»å­¦çŸ¥è¯†ç”Ÿæˆã€‚
âš ï¸ **é‡è¦æç¤º**ï¼šAI ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒï¼Œå¯èƒ½å­˜åœ¨è¯¯å·®ï¼Œè¯·ä»¥ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­ä¸ºå‡†ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"""

        if self._llm_provider == "mock":
            # Mock æ¨¡å¼ï¼šæ— çŸ¥è¯†å›¾è°±æ—¶è¿”å›æç¤ºï¼Œæœ‰çŸ¥è¯†å›¾è°±æ—¶è¿”å›æ¨¡æ¿
            if kg_available:
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
            else:
                answer = await self._generate_fallback_response(request.query, entities)
        elif self._llm_provider == "siliconflow":
            # æ ¹æ®çŸ¥è¯†å›¾è°±æ˜¯å¦æœ‰æ•°æ®é€‰æ‹©ä¸åŒçš„ prompt
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åŒ»ç–—å¥åº·å»ºè®®ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åŒ»ç–—å¥åº·å»ºè®®ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"

            try:
                # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
                messages = [{"role": "system", "content": system_content}]
                messages.extend(history_messages)
                messages.append({"role": "user", "content": prompt})

                # æ·»åŠ è¶…æ—¶æ§åˆ¶
                response = await asyncio.wait_for(
                    self.siliconflow_client.chat.completions.create(
                        model=settings.siliconflow_model,
                        messages=messages,
                        temperature=0.3,
                        max_tokens=2000
                    ),
                    timeout=self.LLM_TIMEOUT
                )
                answer = response.choices[0].message.content

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    answer += no_kg_notice

            except asyncio.TimeoutError:
                logger.warning(
                    f"SiliconFlow call timed out after {self.LLM_TIMEOUT}s, using fallback")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
            except Exception as e:
                logger.error(f"SiliconFlow generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
        elif self._llm_provider == "gemini":
            # æ ¹æ®çŸ¥è¯†å›¾è°±æ˜¯å¦æœ‰æ•°æ®é€‰æ‹©ä¸åŒçš„ prompt
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_prefix = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_prefix = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å›ç­”é—®é¢˜ã€‚"

            try:
                loop = asyncio.get_event_loop()
                response = await asyncio.wait_for(
                    loop.run_in_executor(
                        None,
                        lambda: self.gemini_model.generate_content(
                            f"{system_prefix}\n\n{prompt}",
                            generation_config={
                                "temperature": 0.3,
                                "max_output_tokens": 2000,
                            }
                        )
                    ),
                    timeout=self.LLM_TIMEOUT
                )
                answer = response.text

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    answer += no_kg_notice

            except asyncio.TimeoutError:
                logger.warning(
                    f"Gemini call timed out after {self.LLM_TIMEOUT}s, using fallback")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
            except Exception as e:
                logger.error(f"Gemini generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
        else:  # openai
            # æ ¹æ®çŸ¥è¯†å›¾è°±æ˜¯å¦æœ‰æ•°æ®é€‰æ‹©ä¸åŒçš„ prompt
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"

            try:
                messages = [{"role": "system", "content": system_content}]
                messages.extend(history_messages)
                messages.append({"role": "user", "content": prompt})

                response = await asyncio.wait_for(
                    self.openai_client.chat.completions.create(
                        model="gpt-4",
                        messages=messages,
                        temperature=0.3,
                        max_tokens=2000
                    ),
                    timeout=self.LLM_TIMEOUT
                )
                answer = response.choices[0].message.content

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    answer += no_kg_notice

            except asyncio.TimeoutError:
                logger.warning(
                    f"OpenAI call timed out after {self.LLM_TIMEOUT}s, using fallback")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
            except Exception as e:
                logger.error(f"OpenAI generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)

        # Calculate processing time
        processing_time = int((time.time() - start_time) * 1000)

        # Calculate overall confidence
        confidence_scores = [ev.confidence for ev in evidence if ev.confidence]
        overall_confidence = sum(
            confidence_scores) / len(confidence_scores) if confidence_scores else 0.7

        # Build warnings
        warnings = []
        if not evidence:
            warnings.append("æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„åŒ»å­¦æ–‡çŒ®")
        if not kg_available:
            warnings.append("çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        if not kg_service.is_connected:
            warnings.append("çŸ¥è¯†å›¾è°±æœåŠ¡æœªè¿æ¥")

        # Determine answer source
        if self._llm_provider == "mock":
            if kg_available:
                answer_source = AnswerSource.KNOWLEDGE_GRAPH
            else:
                answer_source = AnswerSource.TEMPLATE
        else:
            if kg_available:
                answer_source = AnswerSource.MIXED  # çŸ¥è¯†å›¾è°± + LLM
            else:
                answer_source = AnswerSource.LLM_ONLY  # çº¯ LLM ç”Ÿæˆ

        # Standard disclaimer
        disclaimer = "âš ï¸ é‡è¦æç¤ºï¼šæœ¬ç³»ç»Ÿä»…ä¾›åŒ»ç–—ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚ç´§æ€¥æƒ…å†µè¯·æ‹¨æ‰“æ€¥æ•‘ç”µè¯ã€‚"

        # If user_id present, store a short memory snippet (non-blocking)
        try:
            if request.user_id:
                mem_content = f"Q: {request.query}\nA: {answer[:1000]}"
                asyncio.create_task(memory_service.store_memory(
                    request.user_id, mem_content, {"query_id": query_id}))
        except Exception as e:
            logger.debug(f"Failed to store memory: {e}")

        return QueryResponse(
            query_id=query_id,
            answer=answer,
            answer_source=answer_source,
            evidence=evidence[:request.max_answers],
            kg_paths=kg_paths,
            confidence_score=round(overall_confidence, 2),
            warnings=warnings,
            disclaimer=disclaimer,
            processing_time_ms=processing_time,
            model_used={"mock": "mock-llm", "gemini": "gemini-1.5-flash", "openai": "gpt-4",
                        "siliconflow": settings.siliconflow_model}.get(self._llm_provider, "mock-llm")
        )

    async def process_query_stream(self, request: QueryRequest) -> AsyncGenerator[str, None]:
        """
        æµå¼å¤„ç†åŒ»ç–—é—®ç­”æŸ¥è¯¢ï¼Œé€æ­¥è¿”å›LLMç”Ÿæˆçš„å†…å®¹

        Yields:
            SSEæ ¼å¼çš„å­—ç¬¦ä¸²æ•°æ®
        """
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        if self._semaphore is None:
            self._semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_REQUESTS)

        async with self._semaphore:
            async for chunk in self._process_query_stream_internal(request):
                yield chunk

    async def _process_query_stream_internal(self, request: QueryRequest) -> AsyncGenerator[str, None]:
        """æµå¼å¤„ç†æŸ¥è¯¢çš„å†…éƒ¨æ–¹æ³•"""
        start_time = time.time()
        query_id = f"q_{uuid.uuid4().hex[:12]}"

        logger.info(
            f"Processing stream query [{query_id}]: {request.query[:50]}...")

        # å‘é€å¼€å§‹çŠ¶æ€
        yield f"data: {json.dumps({'status': 'searching', 'message': 'æ­£åœ¨æ£€ç´¢çŸ¥è¯†å›¾è°±...'}, ensure_ascii=False)}\n\n"

        # Step 1: Extract entities from query (ç»“åˆå†å²è¿›è¡Œæ¶ˆè§£)
        entities = await self._extract_entities_from_kg(request.query, request.history)
        logger.debug(f"Extracted entities: {entities}")

        # Step 2: Retrieve from Knowledge Graph
        kg_paths = []
        if request.include_kg_paths and entities:
            kg_paths = await kg_service.find_paths_for_query(entities)
            logger.debug(f"Found {len(kg_paths)} KG paths")

        # Step 3: Retrieve from Vector Store
        evidence = []
        if request.include_evidence:
            # Use entities extracted from the current query only to avoid pulling evidence from prior turns
            current_entities = await self._extract_entities_from_query_only(request.query)
            evidence = await vector_service.search_documents(
                request.query,
                keywords=current_entities,
                limit=request.max_answers + 2
            )
            logger.debug(
                f"Extracted current-only entities for evidence: {current_entities}")
            logger.debug(f"Found {len(evidence)} evidence documents")

        # å‘é€è¯æ®æ‰¾åˆ°çŠ¶æ€
        yield f"data: {json.dumps({'status': 'evidence_found', 'count': len(evidence)}, ensure_ascii=False)}\n\n"

        # Step 4: Build context for LLM
        kg_context = ""
        if entities and kg_service.is_connected:
            kg_context = await kg_service.get_kg_context_for_query(entities)

        if not kg_context and kg_paths:
            kg_context = "ç›¸å…³åŒ»å­¦çŸ¥è¯†ï¼š\n"
            for path in kg_paths[:3]:
                for node in path.nodes:
                    kg_context += f"- {node.type}: {node.label}"
                    if node.properties.get("description"):
                        kg_context += f" - {node.properties['description']}"
                    kg_context += "\n"

        evidence_context = ""
        if evidence:
            evidence_context = "åŒ»å­¦æ–‡çŒ®è¯æ®ï¼š\n"
            for ev in evidence[:5]:
                evidence_context += f"- [{ev.source}] {ev.snippet}\n"

        # åˆ¤æ–­çŸ¥è¯†å›¾è°±æ˜¯å¦æœ‰æ•°æ®
        kg_available = bool(kg_context or kg_paths)

        # æ„å»ºæ¶ˆæ¯å†å²åˆ—è¡¨
        history_messages = []
        if request.history:
            for msg in request.history[-6:]:
                history_messages.append(
                    {"role": msg.role, "content": msg.content})

        # è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°
        model_name_map = {
            "mock": "æ¨¡æ¿å›å¤",
            "gemini": "Gemini",
            "openai": "GPT-4",
            "siliconflow": settings.siliconflow_model
        }
        current_model_name = model_name_map.get(self._llm_provider, "AI")

        # æ— çŸ¥è¯†å›¾è°±æ•°æ®æ—¶çš„æ¥æºæ ‡æ³¨
        no_kg_notice = f"""

---
ğŸ¤– **æ¥æºè¯´æ˜**ï¼šçŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæœ¬å›ç­”ç”± AI å¤§æ¨¡å‹ï¼ˆ{current_model_name}ï¼‰åŸºäºé€šç”¨åŒ»å­¦çŸ¥è¯†ç”Ÿæˆã€‚
âš ï¸ **é‡è¦æç¤º**ï¼šAI ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒï¼Œå¯èƒ½å­˜åœ¨è¯¯å·®ï¼Œè¯·ä»¥ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­ä¸ºå‡†ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚"""

        # å‘é€å¼€å§‹ç”ŸæˆçŠ¶æ€
        yield f"data: {json.dumps({'status': 'generating', 'message': 'æ­£åœ¨ç”Ÿæˆå›ç­”...'}, ensure_ascii=False)}\n\n"

        full_answer = ""

        if self._llm_provider == "mock":
            # Mock æ¨¡å¼ï¼šæ— æ³•æµå¼ï¼Œç›´æ¥è¿”å›å®Œæ•´å›ç­”
            if kg_available:
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
            else:
                answer = await self._generate_fallback_response(request.query, entities)

            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            for char in answer:
                yield f"data: {json.dumps({'status': 'content', 'text': char}, ensure_ascii=False)}\n\n"
                full_answer += char
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ

        elif self._llm_provider == "siliconflow":
            # SiliconFlow æµå¼è¾“å‡º
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åŒ»ç–—å¥åº·å»ºè®®ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åŒ»ç–—å¥åº·å»ºè®®ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"

            try:
                messages = [{"role": "system", "content": system_content}]
                messages.extend(history_messages)
                messages.append({"role": "user", "content": prompt})

                # æµå¼è°ƒç”¨
                stream = await self.siliconflow_client.chat.completions.create(
                    model=settings.siliconflow_model,
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2000,
                    stream=True
                )

                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_answer += content
                        yield f"data: {json.dumps({'status': 'content', 'text': content}, ensure_ascii=False)}\n\n"

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    full_answer += no_kg_notice
                    yield f"data: {json.dumps({'status': 'content', 'text': no_kg_notice}, ensure_ascii=False)}\n\n"

            except Exception as e:
                logger.error(f"SiliconFlow stream generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
                yield f"data: {json.dumps({'status': 'content', 'text': answer}, ensure_ascii=False)}\n\n"
                full_answer = answer

        elif self._llm_provider == "gemini":
            # Gemini æµå¼è¾“å‡º
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_prefix = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_prefix = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å›ç­”é—®é¢˜ã€‚"

            try:
                loop = asyncio.get_event_loop()
                # Gemini æµå¼è°ƒç”¨
                response = await loop.run_in_executor(
                    None,
                    lambda: self.gemini_model.generate_content(
                        f"{system_prefix}\n\n{prompt}",
                        generation_config={
                            "temperature": 0.3,
                            "max_output_tokens": 2000,
                        },
                        stream=True
                    )
                )

                for chunk in response:
                    if chunk.text:
                        full_answer += chunk.text
                        yield f"data: {json.dumps({'status': 'content', 'text': chunk.text}, ensure_ascii=False)}\n\n"

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    full_answer += no_kg_notice
                    yield f"data: {json.dumps({'status': 'content', 'text': no_kg_notice}, ensure_ascii=False)}\n\n"

            except Exception as e:
                logger.error(f"Gemini stream generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
                yield f"data: {json.dumps({'status': 'content', 'text': answer}, ensure_ascii=False)}\n\n"
                full_answer = answer

        else:  # openai
            # OpenAI æµå¼è¾“å‡º
            if kg_available:
                prompt = self._build_llm_prompt(
                    request.query, kg_context, evidence_context, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»ç–—çŸ¥è¯†å›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"
            else:
                prompt = self._build_llm_prompt_without_kg(
                    request.query, request.history)
                system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚"

            try:
                messages = [{"role": "system", "content": system_content}]
                messages.extend(history_messages)
                messages.append({"role": "user", "content": prompt})

                # æµå¼è°ƒç”¨
                stream = await self.openai_client.chat.completions.create(
                    model="gpt-4",
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2000,
                    stream=True
                )

                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_answer += content
                        yield f"data: {json.dumps({'status': 'content', 'text': content}, ensure_ascii=False)}\n\n"

                # å¦‚æœæ— çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œè¿½åŠ æ¥æºæ ‡æ³¨
                if not kg_available:
                    full_answer += no_kg_notice
                    yield f"data: {json.dumps({'status': 'content', 'text': no_kg_notice}, ensure_ascii=False)}\n\n"

            except Exception as e:
                logger.error(f"OpenAI stream generation failed: {e}")
                answer = self._generate_mock_response(
                    request.query, entities, evidence, kg_paths, kg_context)
                yield f"data: {json.dumps({'status': 'content', 'text': answer}, ensure_ascii=False)}\n\n"
                full_answer = answer

        # Calculate processing time
        processing_time = int((time.time() - start_time) * 1000)

        # Calculate overall confidence
        confidence_scores = [ev.confidence for ev in evidence if ev.confidence]
        overall_confidence = sum(
            confidence_scores) / len(confidence_scores) if confidence_scores else 0.7

        # Build warnings
        warnings = []
        if not evidence:
            warnings.append("æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„åŒ»å­¦æ–‡çŒ®")
        if not kg_available:
            warnings.append("çŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        if not kg_service.is_connected:
            warnings.append("çŸ¥è¯†å›¾è°±æœåŠ¡æœªè¿æ¥")

        # Determine answer source
        if self._llm_provider == "mock":
            if kg_available:
                answer_source = AnswerSource.KNOWLEDGE_GRAPH
            else:
                answer_source = AnswerSource.TEMPLATE
        else:
            if kg_available:
                answer_source = AnswerSource.MIXED
            else:
                answer_source = AnswerSource.LLM_ONLY

        # Standard disclaimer
        disclaimer = "âš ï¸ é‡è¦æç¤ºï¼šæœ¬ç³»ç»Ÿä»…ä¾›åŒ»ç–—ä¿¡æ¯å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚å¦‚æœ‰èº«ä½“ä¸é€‚ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚ç´§æ€¥æƒ…å†µè¯·æ‹¨æ‰“æ€¥æ•‘ç”µè¯ã€‚"

        # æ„å»ºå®Œæ•´å“åº”
        final_response = QueryResponse(
            query_id=query_id,
            answer=full_answer,
            answer_source=answer_source,
            evidence=evidence[:request.max_answers],
            kg_paths=kg_paths,
            confidence_score=round(overall_confidence, 2),
            warnings=warnings,
            disclaimer=disclaimer,
            processing_time_ms=processing_time,
            model_used={"mock": "mock-llm", "gemini": "gemini-1.5-flash", "openai": "gpt-4",
                        "siliconflow": settings.siliconflow_model}.get(self._llm_provider, "mock-llm")
        )

        # Store memory (async) if user_id provided
        try:
            if request.user_id:
                mem_content = f"Q: {request.query}\nA: {full_answer[:1000]}"
                asyncio.create_task(memory_service.store_memory(
                    request.user_id, mem_content, {"query_id": query_id}))
        except Exception as e:
            logger.debug(f"Failed to store memory: {e}")

        # å‘é€å®ŒæˆçŠ¶æ€å’Œå®Œæ•´å“åº”æ•°æ®
        yield f"data: {json.dumps({'status': 'complete', 'response': json.loads(final_response.model_dump_json())}, ensure_ascii=False)}\n\n"


# Singleton instance
qa_service = QAService()
